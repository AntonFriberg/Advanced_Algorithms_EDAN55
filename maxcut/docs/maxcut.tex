\documentclass{tufte-handout}
\usepackage{amsmath,amsthm}

\input{vc.tex}

\usepackage{pgfplots}
\pgfplotsset{width=\textwidth,compat=1.5.1}

\newtheorem{claim}{Claim}[section]
\title{\sf Approximation Algorithm for Maximum Cut}
%\date{\GITAuthorDate}
%\author{Thore Husfeldt}

\begin{document}
\maketitle
\footnotetext{rev. \GITAbrHash}

\section{Maximum Cut}
Consider an undirected graph $G=(V,E)$ with positive edge weights
$w(e)$ $(e\in E)$.
Set $n=|V|$ and $m=|E|$.
The Maximum Cut problem (Maxcut) is to find a partition of the
vertices with the largest total weight of the edges ``crossing'' the
partition, i.e., maximising the value of \[c(A)= \sum_{(u,v)\in E,
  u\in A, v\notin A} w(u,v)\,\] over all $A\subseteq V$.

Maxcut is NP-hard, so we have little hope of writing an algorithm that
solves arbitrary Maxcut instances optimally.

\subsection{ Algorithm R}

Consider the following simple randomized algorithm (call it
Algorithm~R): Let $A$ be a \emph{random subset} of $V$ constructed by
flipping a coin $r(v)\in\{0,1\}$ for every vertex $v\in V$ and setting
$v\in A$ if and only if $r(v)=1$.

\subsection{ Inputs}

The  data directory contains two input instances:
\begin{description}
\item[ pw09\_100.9.txt:] A random instance with $|V|=100$ and
  $|E|=4455$. The best cut in this instance is
  13658.\sidenote{A. Wiegele, Biq Mac Library - A collection of Max-Cut
    and quadratic 0-1 programming instances of medium size, 2007.}
\item[ matching\_1000.txt:] A disjoint union of 500 edges with unit weight.
\end{description}

The input format is straightforward: the first line contains $n$ and
$m$; every following line describes an edge in the format first
vertex, second vertex, weight.
All weights are integers, vertices are numbered $1,2,\ldots, n$.

\subsection{ Deliverables}

\begin{enumerate}
\item Implement algorithm R and run it on the dataset provided in
  the data directory.
  Use whatever programming language and libraries you want, but make
  sure that your code is short and crisp; my own solution takes 25
  lines.
  Attach a printout to the report or have it checked by your lab
  assistant.
\item Fill out the report on the next page; you can just use the
  \LaTeX\ code if you want.
\end{enumerate}

\newpage


\newpage
\section{Maxcut Lab Report}


by Anton Friberg

\subsection{Running time}

The running time of algorithm~R is $[\ldots]$\sidenote{Replace
  $[\ldots]$ by a function of $n$ and/or $m$. You can use asymptotic
  notation. This is supposed to be easy.}.

\subsection{Randomness}

Algorithm R uses $[\ldots]$\sidenote{Replace
  $[\ldots]$ by a function of $n$ and/or $m$. Do not use asymptotic
  notation. This is supposed to be easy.} random bits.

\subsection{Solution quality}

\paragraph{Experiments.}

\begin{enumerate}
\item
For the input file  pw09\_100.9.txt with $t=100$ runs, we found
an average cutsize of $C=12394$, roughly $90.7$\% of the optimum
$\operatorname{OPT} = 13658$.
The distribution of cutsizes looks as follows:

\medskip

\noindent
\begin{tikzpicture}
\begin{axis}[
  height= 5cm,
  ybar interval,
  xmin = 0,  xmax = 13658,
  xtick =       {0   ,   5000,   10000, 13658},
  xticklabels = { $0$, $5000$, $10000$,   OPT},
  x tick label as interval = false,
  scaled ticks = false
]
    \addplot+[hist={bins=100}]
        table[y index=0] {
          % output of 
          % perl -e "for $i (1..100) { system 'python sol/rmaxcut.py < data/pw09_100.9.txt '}"
12392
12275
12020
12639
12501
12592
12587
12297
12470
12463
12663
12444
12498
12475
12158
12432
12410
12308
12383
12544
12659
12261
12368
12404
12539
12583
11839
12566
12180
12256
12264
12453
12309
12453
12473
12448
12349
12545
12287
12360
12619
12539
12351
12601
12378
12304
12342
12483
12497
12543
12391
12384
12466
12386
12447
12430
12426
12359
12208
12410
12312
12571
12582
12534
12554
12526
12538
12566
12375
12465
12582
12331
12249
12234
12438
12411
12537
12073
12395
12648
12258
11971
12516
12551
12295
12390
12475
12489
12323
12529
12095
11706
12455
12176
12354
12198
12216
12225
12302
12341
    };
\end{axis}
\end{tikzpicture}

\item
For the input file matching\_1000.txt with $t=100$ runs, we found an
average cutsize of $C=249$, roughly $50$\% of the optimum $OPT=500$.
The optimum was determined by realizing that the input is a worst case
scenario for our algorithm, and the optimum is to include every edge.
The distribution of cutsizes looks as follows:\sidenote{Display your
  cutsizes as a histogram. Use whatever software you like to produce
  the image; the placeholder image on the left is constructed in the
  \LaTeX\ source.}

\medskip

\noindent
\begin{tikzpicture}
\begin{axis}[
  height= 5cm,
  ybar interval,
  xmin = 0,  xmax = 500,
  xtick =       {0   ,   150,   250, 500},
  xticklabels = { $0$, $150$, $250$,   OPT},
  x tick label as interval = false,
  scaled ticks = false
]
    \addplot+[hist={bins=100}]
        table[y index=0] {
          % output of
          % perl -e "for $i (1..100) { system 'python sol/rmaxcut.py < data/matching\_1000.txt '}"
234
249
226
239
256
257
269
253
250
260
244
245
259
249
249
243
267
261
254
252
260
266
259
237
258
247
254
240
264
259
231
263
239
244
234
262
257
225
260
265
243
234
225
261
253
246
267
241
244
248
243
253
250
253
252
252
237
258
246
233
241
242
240
247
250
263
257
248
249
253
258
256
253
235
248
259
239
246
242
254
260
239
262
250
264
253
237
252
243
238
272
251
250
250
254
259
262
243
263
229
    };
\end{axis}
\end{tikzpicture}

\item
\end{enumerate}
\paragraph{Analysis of performance guarantee}

Clearly, Algorithm R performs quite badly on input 
  matching\_1000.txt.
We will show that it can perform \emph{no worse} than that, i.e., we
will establish that in expectation, the cutsize $C$ satisfies $C \geq
\frac{1}{2}\cdot \operatorname{OPT}$.\sidenote{Replace [\ldots] by the
  right constant}


We will view $C$ as a random variable that gives the size of the cut
defined by the random choices.
Let $W$ denote the total weight of the edges of $G$, i.e.,
\[ W= \sum_{e\in E} w(e)\,.\]

Then,
\begin{equation}\label{eq: E[C]}
E[C] = \textstyle\frac{1}{2}\cdot W\,.
\end{equation}

To see this, define the indicator random variable $X_{uv}$ for every
edge $uv\in E$ as follows.
Set $X_{uv}=1$ if $uv$ crosses the cut, i.e., $u\in A$ and $v\notin A$
or $u\notin A$ and $v\in A$.
Otherwise, $X_{uv} = 0$.

Then, $\Pr(X_{uv} = 1) = \frac{1}{2}$ because $X_{uv}=1$ and $X_{uv}=0$
are disjoint with equal probabilities.
Now, $E[C]= \sum{X_{uv} \cdot w(e)}$ Finally, we have
\(E[C]\geq \frac{1}{2}\cdot \text{OPT}\) because clearly
every cut has a size of at least $\frac{1}{2}\cdot W$ and we want as
many cuts as possible since all weights are positive.


\newpage
\section{Optional: Derandomising Algorithm R}

\subsection{Algorithm L} 


We now reduce the number of random bits used by the algorithm to $\log
n$ using a simple \emph{pseudorandom generator}.


Let $k=\lceil\log (n+1)\rceil$ and flip $k$ coins $b_1,\ldots, b_k$.
There are $2^k -1 \geq n$ different ways of choosing a nonempty subset
$S\subseteq [k]$ of the coins.
Each of these ways defines a random bit $r_S =\bigoplus_{i\in S} b_i$.
(Here, $\oplus$ denotes exclusive or.)
This gives a total of $n$ random bits.
These random bits are not as high-quality as the original $k$ bits,
but they retain the crucial property of \emph{pairwise independence}:
If $S\neq T$ then
\[ \Pr(r_S\neq r_T) = [\ldots],.\]

Extend Algorithm~R using this idea; call the resulting
algorithm~L (for logarithmic randomness).

\subsection{Algorithm Z}

For our final trick, we let the random bits disappear completely:
since Algorithm~L uses only $k$ bits of randomness, we can iterate
over \emph{all} coin flips---there are only $2^k$, which is polynomial
(in fact, linear) in $n$.
Extend algorithm~L using this idea; call the resulting algorithm~Z
(for zero randomness).
The running time of Z is $O([\ldots])$.

\newpage
\section{Perspective}

This lab establishes minimal skills in algorithms implementation,
probabilistic analysis of algorithms (independence, linearity of
expectation, and in particular the trick of computing an expectation
using indicator random variables), and approximation guarantees (in
particular, finding upper and lower bounds by exhibiting a concrete
``bad instance'' and a comparison to a hypothetical optimum,
respectively).
The histogram aims to establish the intuition that measure is
concentrated around its expectation.

\bigskip

To establish that Maxcut is NP-hard one reduces from NAE-Sat, a
reduction that can be found in many places\sidenote{C. Moore and
S. Mertens, \emph{The Nature of Computation}, Oxford University Press,
  2011, p. 146.}
Recall that the related problem \emph{Minimum Cut} is easy because of
the max flow--min cut theorem.
A moment's thought should convince you that as soon as negative
weights are allowed, the two problems are the same (and both are
hard).
Algorithm R doesn't work at all for negative weights.

Algorithm R is a classical randomised approximation algorithm, its
origins seem to be shrouded in the mists of time.
The \emph{deterministic} algorithm of Sahni and Gonzales\sidenote{S.\
  Sahni and T.\ Gonzalez.
  P-complete approximation problems.
  \emph{J.\ Assoc.\ Comput.\ Mach.}, 23(3):555--565, 1976.}
can be viewed as a derandomisation of R using the \emph{method of
  conditional expectations}.
These algorithms were best knows until the breakthrough result of
Goemans and Williamson,\sidenote{M.\ X.\ Goemans and D.\ P.\
  Williamson.
  Improved approximation algorithms for maximum cut and satisfiability
  problems using semidefinite programming.
  \emph{J.\ Assoc.\ Comput.\ Mach.}, 42(6):1115--1145, 1995.}
which improved the approximation factor to $0.87856$.
H\aa{}stad has shown that no algorithm can approximate the maxcut
better than $16/17\sim 0.941176$ unless P equals NP. Khot has shown
that the Goemans--Williamson bound is essentially optimal under the
\emph{Unique Games Conjecture}.

Algorithm L can also be viewed as an application of \emph{pairwise
  independent hash functions}.


\end{document}
